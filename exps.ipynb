{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80707a1-2f16-4bd2-8774-bcc3b6347ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gp import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gp\n",
    "importlib.reload(gp)\n",
    "from gp import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4f9b2",
   "metadata": {},
   "source": [
    "# Dataset characteristics<br>\n",
    "- instant: record index<br>\n",
    "- dteday : date\n",
    "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month (1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : whether day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccf28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hour.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61bab1-6888-48b2-ad1e-09737697ae28",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f615238",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = pd.Timestamp('2011-01-01')\n",
    "df.iloc[:, 1] = (pd.to_datetime(df.iloc[:, 1]) - reference_date).dt.days\n",
    "data = df.to_numpy(dtype=float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_whole = data[:, 1:14] # the first column is index, not necessary\n",
    "Y_whole = data[:, 16] # count of total rental bikes as label\n",
    "X_whole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    return (X-mean)/std, mean, std\n",
    "\n",
    "def split_train_test(X_whole, Y_whole, num=1500, rate=0.7):\n",
    "    # Shuffle data first\n",
    "    indices = jax.random.permutation(grab_prng(), X_whole.shape[0])\n",
    "    X_part = X_whole[indices]\n",
    "    Y_part = Y_whole[indices]\n",
    "    \n",
    "    indices = jax.random.permutation(grab_prng(), num)\n",
    "    X_shuffled = X_part[indices]\n",
    "    Y_shuffled = Y_part[indices]\n",
    "    \n",
    "    # Standardize the shuffled data\n",
    "    X, mean, std = standardize(X_shuffled)\n",
    "    Y, mean, std = standardize(Y_shuffled)\n",
    "\n",
    "    # Use the same indices for Y\n",
    "    # Y = Y_shuffled\n",
    "    \n",
    "    # Compute the split index\n",
    "    split_idx = int(num * rate)\n",
    "    \n",
    "    # Split indices into train and test\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    \n",
    "    # Split data using the indices\n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clml(X_train, Y_train, ratio=0.8):\n",
    "    n_samples = X_train.shape[0]\n",
    "    indices = jax.random.permutation(grab_prng(), n_samples)\n",
    "    # Shuffle the data\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    Y_train_shuffled = Y_train[indices]\n",
    "    \n",
    "    # Calculate the base size of each subset\n",
    "    subset_size = int(n_samples * ratio)\n",
    "    \n",
    "    # Initialize lists to hold the subsets\n",
    "    X_con = [X_train_shuffled[:subset_size], X_train_shuffled[subset_size:]]\n",
    "    Y_con = [Y_train_shuffled[:subset_size], Y_train_shuffled[subset_size:]]\n",
    "    \n",
    "    return X_con, Y_con\n",
    "\n",
    "def split_sequence(X_train, Y_train, seq_len=8):\n",
    "    n_samples = X_train.shape[0]\n",
    "    indices = jax.random.permutation(grab_prng(), n_samples)\n",
    "     # Shuffle the data\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    Y_train_shuffled = Y_train[indices]\n",
    "    \n",
    "    # Calculate the base size of each subset\n",
    "    subset_size = n_samples // seq_len\n",
    "    remainder = n_samples % seq_len\n",
    "    \n",
    "    # Initialize lists to hold the subsets\n",
    "    X_seq = []\n",
    "    Y_seq = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for i in range(seq_len):\n",
    "        # The first subset gets the remainder (extra) samples\n",
    "        if i == 0:\n",
    "            end_idx = start_idx + subset_size + remainder\n",
    "        else:\n",
    "            end_idx = start_idx + subset_size\n",
    "        \n",
    "        X_seq.append(X_train_shuffled[start_idx:end_idx])\n",
    "        Y_seq.append(Y_train_shuffled[start_idx:end_idx])\n",
    "        \n",
    "        start_idx = end_idx\n",
    "    return X_seq, Y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b21180-e161-471e-b974-5af0442e297f",
   "metadata": {},
   "source": [
    "# Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b728b-22b4-49d4-a3da-fa1da6cb7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(lml_lpd, label_x, label_y, tit):\n",
    "    plt.scatter(lml_lpd[:,0], lml_lpd[:,1])\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "    plt.title(tit)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#\n",
    "\n",
    "def plot_comparision(lml, clml, clmls):\n",
    "    plt.scatter(range(len(lml)), lml, label=\"LML\", color=\"red\", marker=\"o\")\n",
    "    plt.scatter(range(len(clml)), clml, label=\"CLML\", color=\"blue\", marker=\"x\")\n",
    "    plt.scatter(range(len(clmls)), clmls, label=\"CLMLS\", color=\"green\", marker=\"s\")\n",
    "\n",
    "    plt.xlabel(\"Hyperparameter Setting Index\")\n",
    "    plt.ylabel(\"NLL\")\n",
    "    plt.title(\"Comparison Between Different Generalization Metrics\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c1238-32ba-4f43-94f7-962d8e350a01",
   "metadata": {},
   "source": [
    "# Experiment 2 - feature importance using learned Mahalanobis distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8d5ee-5be9-4a80-a97a-4d7b93673d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_sqexp_maha_hyperparams(X_train):\n",
    "    var = jax.random.uniform(grab_prng(), shape=(2,), minval=0.2, maxval=2)\n",
    "    length_scale = jax.random.uniform(grab_prng(), shape=(X_train.shape[1],), minval=1, maxval=10)\n",
    "    return np.concatenate([var, length_scale])\n",
    "\n",
    "def lml_exp(hyperparams_inits, T=200, step_size=1e-4):\n",
    "    lml_trials = []\n",
    "    for n in range(n_trials):\n",
    "        hyperparams_init = hyperparams_inits[n]\n",
    "        unconstrained_hyperparams_init = inverse_param_transform(hyperparams_init)\n",
    "        (unconstrained_hyperparams, lml) = empirical_bayes(cov_func, X_train, Y_train, unconstrained_hyperparams_init, step_size, T)\n",
    "        hyperparams = param_transform(unconstrained_hyperparams)\n",
    "        \n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams_init)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_initial = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams_init[0])\n",
    "\n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_final = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams[0])\n",
    "        print(np.array([neg_log_initial, neg_log_final, lml]))\n",
    "        lml_trials.append(np.array([neg_log_initial, neg_log_final, lml]))\n",
    "    # #\n",
    "    trials_array = np.array(lml_trials)\n",
    "    lml_nlpd = np.column_stack((trials_array[:, 2], trials_array[:, 1]))\n",
    "    return lml_nlpd\n",
    "\n",
    "def clml_exp(hyperparams_inits, T=200, step_size=1e-3):\n",
    "    clml_trials = []\n",
    "    for n in range(n_trials):\n",
    "        hyperparams_init = hyperparams_inits[n]\n",
    "        unconstrained_hyperparams_init = inverse_param_transform(hyperparams_init)\n",
    "        (unconstrained_hyperparams, clml) = clml_opt(cov_func, X_con, Y_con, unconstrained_hyperparams_init, step_size, T)\n",
    "        hyperparams = param_transform(unconstrained_hyperparams)\n",
    "\n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams_init)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_initial = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams_init[0])\n",
    "\n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_final = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams[0])\n",
    "        print(np.array([neg_log_initial, neg_log_final, clml]))\n",
    "        clml_trials.append(np.array([neg_log_initial, neg_log_final, clml]))\n",
    "\n",
    "    #\n",
    "    trials_array = np.array(clml_trials)\n",
    "    clml_nlpd = np.column_stack((trials_array[:, 2], trials_array[:, 1]))\n",
    "    return clml_nlpd\n",
    "\n",
    "def clmls_exp(hyperparams_inits, T=200, step_size=5e-3):\n",
    "    clmls_trials = []\n",
    "    for n in range(n_trials):\n",
    "        hyperparams_init = hyperparams_inits[n]\n",
    "        unconstrained_hyperparams_init = inverse_param_transform(hyperparams_init)\n",
    "        (unconstrained_hyperparams, clmls) = clmls_opt(cov_func, X_seq, Y_seq, unconstrained_hyperparams_init, step_size, T)\n",
    "        hyperparams = param_transform(unconstrained_hyperparams)\n",
    "        \n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams_init)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_initial = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams_init[0])\n",
    "\n",
    "        posterior_predictive = gp_posterior(cov_func, X_train, Y_train, hyperparams)\n",
    "        (posterior_mean, posterior_var) = posterior_predictive(X_star)\n",
    "        neg_log_final = neg_log_predictive_density(Y_test, posterior_mean, posterior_var, hyperparams[0])\n",
    "        print(np.array([neg_log_initial, neg_log_final, clmls]))\n",
    "        clmls_trials.append(np.array([neg_log_initial, neg_log_final, clmls]))\n",
    "    #\n",
    "    trials_array = np.array(clmls_trials)\n",
    "    clmls_nlpd = np.column_stack((trials_array[:, 2], trials_array[:, 1]))\n",
    "    return clmls_nlpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment settings\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = split_train_test(X_whole, Y_whole, num=3000, rate=0.7)\n",
    "X_con, Y_con = split_clml(X_train, Y_train, ratio=0.8)\n",
    "X_seq, Y_seq = split_sequence(X_train, Y_train, seq_len=10)\n",
    "\n",
    "n_trials = 20\n",
    "cov_func = sqexp_mahalanobis_cov_function\n",
    "X_star = X_test\n",
    "hyperparams_inits = []\n",
    "for i in range(n_trials):\n",
    "    hyperparams_inits.append(random_init_sqexp_maha_hyperparams(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lml_nlpd = lml_exp(hyperparams_inits, T=200, step_size=1e-3)\n",
    "scatterplot(lml_nlpd, 'LML', 'NLPD', 'Generalization of LML')\n",
    "clml_nlpd = clml_exp(hyperparams_inits, T=200, step_size=5e-3)\n",
    "scatterplot(clml_nlpd, 'CLML', 'NLPD', 'Generalization of CLML')\n",
    "clmls_nlpd = clmls_exp(hyperparams_inits, T=200, step_size=5e-3)\n",
    "scatterplot(clmls_nlpd, 'CLMLS', 'NLPD', 'Generalization of CLMLS')\n",
    "plot_comparision(lml_nlpd[:, 1], clml_nlpd[:, 1], clmls_nlpd[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cbcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55834f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gp\n",
    "importlib.reload(gp)\n",
    "from gp import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_hyperparams=inverse_param_transform(hyperparams_inits[0])\n",
    "\n",
    "clmls_function = conditional_log_marginal_likelihood_sequence(cov_func, X_seq, Y_seq, whole_sequence=True)\n",
    "clmls_vals=clmls_function(unconstrained_hyperparams)\n",
    "clmls_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b =opt_clmls(clmls_vals)\n",
    "print(a,b)\n",
    "x_data = np.arange(len(clmls_vals))\n",
    "y_data = np.array(clmls_vals)\n",
    "x_fit = np.linspace(min(x_data), max(x_data), 100)  # Smooth x range for the fitted curve\n",
    "y_fit = a + b * x_fit\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_data, y_data, color='blue', label='Sequence', zorder=5)\n",
    "\n",
    "# Plot the fitted model\n",
    "plt.plot(x_fit, y_fit, color='red', label='Regression Model', zorder=4)\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.xlabel('CLMLS index')\n",
    "plt.ylabel('CLMLS value')\n",
    "plt.title('CLMLS fitting')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
